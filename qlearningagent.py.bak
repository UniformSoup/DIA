from agent import Agent
import numpy as np

class QLearningAgent(Agent):
    def __init__(self, file="qlearning.npy", isLearning=False):
        self.file = file        
        self.isLearning = isLearning


        self.intervals = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45]
        self.actions = [-0.3, -0.2, -0.1, 0.0, 0.1, 0.2, 0.3]
        self.learningRate = 0.3 # Alpha
        self.discountFactor = 0.975 # Gamma
        self.explorationRate = 0.3 # Epsilon
        self.decayRate = 0.99 # Decay Rate
        self.state = None
        self.direction = None

        try:
            self.qtable = np.load(file)
        except:
            self.qtable = np.zeros((len(self.intervals) + 1, len(self.intervals) + 1, len(self.intervals) + 1, len(self.intervals) + 1, len(self.intervals) + 1, len(self.actions)))
            np.save(file, self.qtable)

    def getState(self, track):
        left = np.digitize(np.min(track[:9]) * 3.0, self.intervals)
        middle = np.digitize(track[9] * 2.0, self.intervals)
        right = np.digitize(np.min(track[10:]) * 3.0, self.intervals)

        return (left, middle, right)
    
    def updateTable(self, state, action, reward, next_state):
        oldQvalue = self.qtable[state][action]
        maxFutureQvalue = np.max(self.qtable[next_state])
        td = reward + self.discountFactor * maxFutureQvalue
        self.qtable[state][action] = self.learningRate * (td - oldQvalue) + oldQvalue

    def action(self, observation, reward, done):
        current_state = self.getState(observation[6])

        if self.isLearning and self.state != current_state and self.state != None:
            self.updateTable(self.state, self.direction, reward, current_state)
            print("State:", self.state)
            print("Reward:", reward)
            print("Exploration Rate:", self.explorationRate)

        if self.isLearning and np.random.rand() < self.explorationRate:
            self.direction = np.random.choice(len(self.actions))
        else:
            self.direction = np.argmax(self.qtable[current_state])

        if done:
            np.save(self.file, self.qtable)
            self.explorationRate *= self.decayRate
            self.state = None
        else:
            self.state = current_state

        throttle = np.clip(observation[6][9] * 3 + 0.25, 0.0, 1.0)

        return (self.actions[self.direction], throttle)